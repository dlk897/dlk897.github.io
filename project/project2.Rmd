---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

# Project 2

## Desiree Kibbee EID: dlk897


### Introducing Data

```{R}

library(AER)
data("Medicaid1986")
data <- Medicaid1986 %>% select(1:5, 8:12) 
head(data)
dim(data)
```
*For this project I chose the built in R dataset "Medicaid1986." It details consumer use of Medicaid in 1986 including the number of visits that individual made to the doctor (visits variable), the number of days needed for ambulatory/outpatient care (exposure variable), number of children in their household (children variable), their age (age variable), their annual household income (income variable), whether they had low (0) or high (1) access to health services (access variable), whether or not they were married (married variable), their gender, ethnicity, number of years they completed of school including college (school variable), and four other variables were originally in the dataset but I dropped them. There are 996 observations in the dataset.*


### MANOVA, ANOVA, t test

```{R}
man1<-manova(cbind(access, income, school, visits, exposure)~ethnicity, data=data)
summary(man1)
summary.aov(man1)
pairwise.t.test(data$income,data$ethnicity, p.adj="none")
pairwise.t.test(data$school,data$ethnicity, p.adj="none")

#total t test
1+5+2
#probability at least one type 1 error
1-(0.95^8)
#bonferroni 
0.05/8
```

*First I ran a MANOVA analysis to test the null hypothesis that for all DVs (access, income, school, visits, exposure), means for "caucasian" and "other" ethnicity are equal. Some assumptions for the MANOVA test include random samples and independent observations which this dataset likely does meet, multivariate normality of DVs which some probably do meet. The other assumptions including: homogeneity of within-group covariance matrices, linear relationship among DVs, extreme outliers, and no multicollinearity, were likely not met. 1 MANOVA, 5 ANOVA, and 2 t-tests were run making 8 total tests. Probability of at least one type I error is 0.34. The bonferroni correction is 0.00625 to keep overall type I error rate at 0.05 does not change the results of our tests in this case. Overall, I found that there is a significant difference in the average income as well as a significant difference in the average years of school completed between "caucasian" and "other" ethnicity.*

### Randomization Test

```{R}

data%>%group_by(gender)%>%
  summarize(means=mean(income))%>%summarize(`mean_diff`=diff(means))

rand_dist<-vector() 

for(i in 1:5000){
new<-data.frame(income=sample(data$income),gender=data$gender)
rand_dist[i]<-mean(new[new$gender=="male",]$income)-   
              mean(new[new$gender=="female",]$income)} 

{hist(rand_dist,main="",ylab=""); abline(v = c(-0.751, 0.751),col="red")}

#p-value
mean(rand_dist>0.751 | rand_dist < -0.751)
```

*For my randomization test the null hypothesis Ho = mean income is the same for females vs. males. My alternative hypothesis Ha = mean income is different for females vs. males. Based on the results we reject the null hypothesis, the mean income is not the same for females vs. males. The probability of observing a mean difference as extreme as the one I got is 0.019 which is less than 0.05.*

### Linear Regression Model

    
```{R}
library(tidyverse)
data$access_c <- data$access - mean(data$access)
fit<-lm(income~ethnicity*access_c, data=data)
summary(fit)

data %>% select(access, income, ethnicity) 
ggplot(data, aes(access,income, color = ethnicity)) + geom_smooth(method = "lm", se = F, fullrange = T) +geom_point()+geom_vline(xintercept=0,lty=2)+geom_vline(xintercept=mean(data$access))

#proportion of variation explained
(sum((data$income-mean(data$income))^2)-sum(fit$residuals^2))/sum((data$income-mean(data$income))^2)

#assumptions
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
ggplot()+geom_histogram(aes(resids), bins=20)
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids))

#Recompute regression results
#uncorrected SE
summary(fit)$coef[,1:2]
#corrected SE
coeftest(fit, vcov = vcovHC(fit))


```

*When ethnicity is "other" and access is average estimated income is 8.7442. When access is average and ethnicity is caucasian the estimated income is 0.7979 less than income estimated for "other" ethnicity. Every one unit increase in access the estimated income for "other" ethnicity decreases by 0.2279. When ethnicity is caucasian and access is not equal to average every one unit increase in access increases estimated income by 0.4481 compared to "other" ethnicity. The proportion of variation explained by this model is 1%, a very bad model. After running the model with robust SEs there is no change in significance because the ethnicity caucasain is still a significant predictor of income. Some standard errors did increase slightly as expected.*

### Bootstrapped SEs


```{R}

fit <- lm(income~ethnicity*access_c, data=data) #fit model
resids<-fit$residuals #save residuals
fitted<-fit$fitted.values #save yhats
resid_resamp<-replicate(5000,{
new_resids<-sample(resids,replace=TRUE) #resample resids w/ replacement
data$new_income<-fitted+new_resids #add new resids to yhats to get new "data"
fit<-lm(new_income~ethnicity*access_c,data=data) #refit model
coef(fit) #save coefficient estimates (b0, b1, etc)
})
resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)

```

*SEs uncorrected from the original model were (intercept) 0.20718, (ethnicitycaucasion) 0.24872, (access_c) 1.11686, and (interaction) 1.34451. Robust SEs were (intercept) 0.22486, (ethnicitycaucasion) 0.26113, (access_c) 1.10046, and (interaction) 1.32362. Bootstrapped SEs were (intercept) 0.20886, (ethnicitycaucasion) 0.25176, (access_c) 1.10316, and (interaction) 1.32457. Between the three different SEs the values didn't change very much. Each SE was within 0.02 of each other. The p-values also did not change significantly between the three different SEs.* 

### Logistic Regression Model Two Variables

    
```{R}
library(tidyverse); library(lmtest)
data$y<-ifelse(data$married=="yes",1,0)
data_fit2 <- data %>% select(-married)
fit2<-glm(y~visits+children,data=data_fit2,family="binomial")
coeftest(fit2)
exp(coef(fit2))%>%round(3)

#confusion matrix
data$prob<-predict(fit2,type="response") 
table(predict=as.numeric(data$prob>.5),truth=data$y)%>%addmargins

#Accuracy
780/996
#sensitivity
0/216
#Specificity
780/780
#precision
0/216

#ggplot
data$logit<-predict(fit2,type="link")
data%>%ggplot()+geom_density(aes(logit,color=married,fill=married), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("predictor (logit)")

#ROC curve plot
library(plotROC) 
#geom_roc needs actual outcome (0,1) and predicted prob (or predictor if just one)
ROCplot<-ggplot(data)+geom_roc(aes(d=y,m=prob), n.cuts=0); ROCplot
calc_auc(ROCplot)


```

*Odds of being married with 0 visits and 0 children is 0.29. Every 1 unit increase in visits increases odds of being married by a factor of 0.98. Every 1 unit increase in children increases odds of being married by a factor of 0.99. Accuracy of this model is 78% meaning for this set of data it will correctly predict marital status 78% of the time. Sensitivity is 0 because this model didn't classify anyone as married. Specificity is 1 the model correctly predicted a marital status of "no" 780/780 times. Precision is 0 because again the model predicted no one as being married. Based on the ROC plot and AUC this model is bad (AUC = 0.50) at predicting marital status from number of children and number of visits to the doctor in a year. There is complete overlap in the density plot further showing it is not a good model.*

### Logistic Regression Model All Variables

    
```{R}
data_fit3 <- data %>% select(-access_c, -prob, -logit, -married)
fit3<-glm(y~.,data=data_fit3,family="binomial")
coeftest(fit3)

data_fit3$probs<-predict(fit3,type="response") 
table(predict=as.numeric(data_fit3$probs>.5),truth=data_fit3$y)%>%addmargins
#accuracy
(734+62)/996
#sensitivity
62/216
#specificity
734/780
#precision
62/108
#AUC
ROCplot2<-ggplot(data_fit3)+geom_roc(aes(d=y,m=probs), n.cuts=0)
calc_auc(ROCplot2)

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

#10-fold CV
set.seed(1234)
k=10 
data<-data_fit3[sample(nrow(data_fit3)),]
folds<-cut(seq(1:nrow(data_fit3)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){

train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$y 

fit4<-glm(y~.,data=data_fit3,family="binomial")

probs2<-predict(fit4,newdata = test,type="response")

diags<-rbind(diags,class_diag(probs2,truth))
}
summarize_all(diags,mean)


#LASSO
data_fit4 <- data_fit3 %>% select(-probs)
library(glmnet)
y<-as.matrix(data_fit4$y) #grab response
x<-model.matrix(y~.,data=data_fit4)[,-1] #grab predictors
head(x)

cv <- cv.glmnet(x,y) #picks an optimal value for lambda through 10-fold CV

{plot(cv$glmnet.fit, "lambda", label=TRUE); abline(v = log(cv$lambda.1se)); abline(v = log(cv$lambda.min),lty=2)}

cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)

#10-fold CV
set.seed(1234)
k=10 
data<-data_fit4[sample(nrow(data_fit4)),]
folds<-cut(seq(1:nrow(data_fit4)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){

train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$y 

fit5<-glm(y~income+gender+school,data=data_fit4,family="binomial")

probs3<-predict(fit5,newdata = test,type="response")

diags2<-rbind(diags,class_diag(probs3,truth))
}
summarize_all(diags2,mean)

```
*For this model accuracy is 80% meaning 80% of the time this model will correctly predict marital status from all the other variables. Sensitivity is 0.29 and is the probability of predicting someone is married and they actually are. Specificity is 0.94 and is the probability of predicting they are not married and they are actually not married. Precision is 0.57 and is the proportion of people classified as married and actually are. AUC for in sample diagnostics is 0.78 so this model is fair at predicting marital status. For the out-of-sample classification diagnostics accuracy increased slightly to 81%, sensitivity increased to 0.32, specificity increased slightly to 0.95, and AUC is the same at 0.78. This means after running our 10 fold CV there is a higher probability of correctly predicting someones marital status and that is their true marital status, creating a slightly better model. After running lasso regression the variables that are non-zero are income, gender, and number of years completed of school. After performing another 10 fold CV using just the variables predicted from lasso, AUC is still the same 0.77. Overall, this model is fair at predicting marital status but not great.* 
...





